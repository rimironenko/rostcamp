# LLM Prompt Application

A Python application for interacting with OpenAI's API to handle prompts, with a focus on modularity, testing, and evaluation.

## ğŸš€ Setup

1. Clone the repository
2. Create and activate a virtual environment:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
4. Create a `.env` file in the root directory with:
   ```
   OPENAI_API_KEY=your-api-key-here
   MODEL_NAME=gpt-3.5-turbo
   ```

## ğŸ’» Usage

Run the CLI application:
```bash
python -m app.main
```

The application will:
1. Prompt you for input
2. Route your prompt through the router
3. Send it to OpenAI
4. Display the response

## ğŸ§ª Testing

Run all tests:
```bash
python -m pytest
```

Run specific test files:
```bash
python -m pytest tests/test_llm_client.py
python -m pytest tests/test_prompts.py
```

## ğŸ“Š Evaluation

The project includes evaluation support using OpenAI's evaluation framework:

- Test cases are in `eval/openai_eval.jsonl`
- Configuration is in `eval/openai_eval_config.yaml`

## ğŸ“ Project Structure

```
llm_app/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # CLI entry point
â”‚   â”œâ”€â”€ config.py        # Environment configuration
â”‚   â”œâ”€â”€ llm_client.py    # OpenAI API interaction
â”‚   â”œâ”€â”€ prompt_router.py # Prompt routing logic
â”‚   â”œâ”€â”€ schemas.py       # Data models
â”‚   â””â”€â”€ utils.py         # Helper functions
â”œâ”€â”€ tests/               # Unit tests
â”œâ”€â”€ eval/                # Evaluation files
â”œâ”€â”€ requirements.txt     # Dependencies
â””â”€â”€ .env                 # Environment variables
```

## ğŸ”§ Development

- All code changes should be tested
- Follow the existing code style
- Add tests for new functionality
- Update documentation as needed
