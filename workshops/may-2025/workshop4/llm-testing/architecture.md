
# LLM Prompt Application Architecture (Python + OpenAI)

## ðŸ—‚ File & Folder Structure

```
llm_app/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                # Entry point of the app
â”‚   â”œâ”€â”€ config.py              # Environment configuration and API keys
â”‚   â”œâ”€â”€ prompt_router.py       # Logic for routing/dispatching prompts
â”‚   â”œâ”€â”€ llm_client.py          # Wrapper for OpenAI API interaction
â”‚   â”œâ”€â”€ schemas.py             # Data models (e.g., Pydantic)
â”‚   â””â”€â”€ utils.py               # Helper functions
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_llm_client.py     # Unit tests for LLM client using mocks
â”‚   â”œâ”€â”€ test_prompts.py        # Test various prompts and responses
â”‚   â””â”€â”€ eval/
â”‚       â”œâ”€â”€ deepeval_test.py   # Quality tests using DeepEval
â”‚       â””â”€â”€ openai_eval.jsonl  # OpenAI Evals formatted testset
â”‚
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ openai_eval_config.yaml  # OpenAI eval configuration
â”‚   â””â”€â”€ prompt_variants/         # Variant prompt examples for evaluation
â”‚       â”œâ”€â”€ task1.jsonl
â”‚       â””â”€â”€ task2.jsonl
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env
```

## ðŸ“¦ What Each Part Does

### `app/`
- **main.py**: Starts the application, defines routes (if CLI or API), and serves as the orchestrator.
- **config.py**: Loads environment variables (e.g., OpenAI API key) and settings.
- **prompt_router.py**: Defines prompt logic â€“ selects or modifies prompt templates based on use case.
- **llm_client.py**: Handles communication with OpenAI's API (e.g., `openai.ChatCompletion.create`).
- **schemas.py**: Defines request/response formats using Pydantic models.
- **utils.py**: Utility functions (e.g., retry logic, logging setup).

### `tests/`
- **test_llm_client.py**: Mocks OpenAI API calls to ensure the client sends correct payloads and handles errors.
- **test_prompts.py**: Tests correctness and expected outcomes of prompt logic.
- **eval/deepeval_test.py**: Uses DeepEval for regression/quality tests (e.g., faithfulness, coherence).
- **eval/openai_eval.jsonl**: Provides prompt-response pairs for evaluation using OpenAI Evals framework.

### `eval/`
- **openai_eval_config.yaml**: Configuration for running OpenAI evals (e.g., eval type, metrics).
- **prompt_variants/**: Structured test cases for various prompts grouped by task.

### Root Files
- **requirements.txt**: Python dependencies.
- **README.md**: Project overview and setup instructions.
- **.env**: API keys and config variables.

## ðŸ”— Service Connections

1. **App Startup (`main.py`)**
   - Loads config â†’ Initializes LLM Client â†’ Routes prompt via `prompt_router.py` â†’ Sends to OpenAI via `llm_client.py`.

2. **Prompt Logic (`prompt_router.py`)**
   - Applies templates/variants depending on use case â†’ Returns structured prompt to `llm_client.py`.

3. **LLM Call (`llm_client.py`)**
   - Authenticated OpenAI call with prompt â†’ Receives and returns response.

4. **Testing:**
   - **Unit Tests** (`test_*.py`): Validate prompt structure, API payloads, and error handling.
   - **DeepEval**: Evaluates generated responses using human-aligned metrics.
   - **OpenAI Evals**: Runs large-scale evaluations with predefined scenarios.

## âœ… Example Evaluation Tools

- **DeepEval**:
  - Use `.deepeval_test.py` to evaluate response coherence, correctness, etc.
- **OpenAI Evals**:
  - CLI: `oaiexperiments evaluate openai_eval_config.yaml`
  - Inputs: `openai_eval.jsonl`

## ðŸ“Œ Notes

- Modular architecture allows reuse of `llm_client` across CLI, web API, or batch apps.
- Easily extendable to support multiple LLMs (e.g., Anthropic, Mistral).
- Testing and eval coverage is decoupled to facilitate high-quality output verification.
